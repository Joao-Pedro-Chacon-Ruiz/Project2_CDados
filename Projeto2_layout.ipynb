{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - CiÃªncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nome: Breno Marti**\n",
    "\n",
    "**Nome: JoÃ£o Pedro Chacon Ruiz**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SerÃ£o permitidos grupos de trÃªs pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarÃ£o fazer um questionÃ¡rio de avaliaÃ§Ã£o de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador automÃ¡tico de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "!pip install emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta: ***[Preencha aqui o id da sua conta. Ex: @fulano ]***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Dados de autenticaÃ§Ã£o do twitter:\\n\\n#Coloque aqui o identificador da conta no twitter: @fulano\\n\\n#leitura do arquivo no formato JSON\\nwith open('auth.pass') as fp:    \\n    data = json.load(fp)\\n\\n#Configurando a biblioteca. NÃ£o modificar\\nauth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\\nauth.set_access_token(data['access_token'], data['access_token_secret'])\\n\""
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Dados de autenticaÃ§Ã£o do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @fulano\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. NÃ£o modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas do projeto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de um produto e coleta das mensagens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'Bugatti'\n",
    "\n",
    "#Quantidade mÃ­nima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade mÃ­nima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de lÃ­ngua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Cria um objeto para a captura\\napi = tweepy.API(auth)\\n\\n#Inicia a captura, para mais detalhes: ver a documentaÃ§Ã£o do tweepy\\ni = 1\\nmsgs = []\\nfor msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode=\"extended\").items():\\n    if (not msg.retweeted) and (\\'RT\\' not in msg.full_text):\\n        msgs.append(msg.full_text.lower())\\n        i += 1\\n    if i > n:\\n        break\\n\\n#Embaralhando as mensagens para reduzir um possÃ­vel viÃ©s\\nshuffle(msgs\\'\\n'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentaÃ§Ã£o do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode=\"extended\").items():\n",
    "    if (not msg.retweeted) and ('RT' not in msg.full_text):\n",
    "        msgs.append(msg.full_text.lower())\n",
    "        i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possÃ­vel viÃ©s\n",
    "shuffle(msgs'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Verifica se o arquivo nÃ£o existe para nÃ£o substituir um conjunto pronto\\nif not os.path.isfile('./{0}.xlsx'.format(produto)):\\n    \\n    #Abre o arquivo para escrita\\n    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\\n\\n    #divide o conjunto de mensagens em duas planilhas\\n    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\\n    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\\n\\n    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\\n    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\\n\\n    #fecha o arquivo\\n    writer.save()\\n\""
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Verifica se o arquivo nÃ£o existe para nÃ£o substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Classificando as mensagens na coragem\n",
    "\n",
    "Esta etapa Ã© manual. FaÃ§a a mesma pelo Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_excel('Bugatti_treino.xlsx')\n",
    "relevante_treino_raw = raw[raw['RelevÃ¢ncia'] == 1]\n",
    "nao_relevante_treino_raw = raw[raw[\"RelevÃ¢ncia\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FunÃ§Ãµes de limpeza de texto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        FunÃ§Ã£o de limpeza muito simples que troca alguns sinais bÃ¡sicos por espaÃ§os\n",
    "    \"\"\"\n",
    "    punctuation = '[#@,!\\()-.â‚¬\":?;]' # Note que os sinais [] sÃ£o delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    return text_subbed   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_emojis(text):\n",
    "    clean = emojis.decode(text)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['\\n','https','//t','co/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpando e transformando os dataframes em sÃ©ries para a anÃ¡lise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "for tweet in relevante_treino_raw['Treinamento']:\n",
    "    texto = transform_emojis(tweet)\n",
    "    list1.append(cleanup(texto))\n",
    "    \n",
    "relevante_treino = pd.Series(list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota do professor: explicar como o cÃ³digo abaixo funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_relevante_treino = relevante_treino.str.replace('|'.join(l),'',regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = []\n",
    "for tweet in nao_relevante_treino_raw['Treinamento']:\n",
    "    texto = transform_emojis(tweet)\n",
    "    list2.append(cleanup(texto))\n",
    "    \n",
    "nao_relevante_treino = pd.Series(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_nao_relevante_treino = nao_relevante_treino.str.replace('|'.join(l),'',regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "list3 = []\n",
    "for tweet in raw['Treinamento']:\n",
    "    texto = transform_emojis(tweet)\n",
    "    list3.append(cleanup(texto))\n",
    "    \n",
    "tudo = pd.Series(list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_treino = tudo.str.replace('|'.join(l),'',regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contando o nÃºmero de apariÃ§Ãµes de cada palavra :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list4 = []\n",
    "for tweet in tweets_treino:\n",
    "    list4 = list4 + tweet.split()\n",
    "\n",
    "palavras_universo_treino = pd.Series(list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list5 = []\n",
    "for tweet in tweets_relevante_treino:\n",
    "    list5 = list5 + tweet.split()\n",
    "\n",
    "palavras_relevantes_treino = pd.Series(list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list6 = []\n",
    "for tweet in tweets_nao_relevante_treino:\n",
    "    list6 = list6 + tweet.split()\n",
    "\n",
    "palavras_nao_relevantes_treino = pd.Series(list6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FrequÃªncia relativa de cada palavra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bugatti         0.044511\n",
       "de              0.025549\n",
       "a               0.024750\n",
       "o               0.023752\n",
       "que             0.014970\n",
       "um              0.014770\n",
       "e               0.013772\n",
       "do              0.013373\n",
       "chiron          0.012974\n",
       "nÃ£o             0.012375\n",
       "na              0.010978\n",
       "carro           0.009182\n",
       "km/h            0.009182\n",
       "nome            0.007984\n",
       "490             0.007984\n",
       "eu              0.007784\n",
       "Ã©               0.007784\n",
       "com             0.007385\n",
       "em              0.006986\n",
       "primeira        0.006587\n",
       "mais            0.006587\n",
       "seu             0.005988\n",
       "gente           0.005190\n",
       "letra           0.005190\n",
       "para            0.005190\n",
       "por             0.004990\n",
       "velocidade      0.004790\n",
       "da              0.004790\n",
       "recorde         0.004591\n",
       "bananaobjeto    0.004591\n",
       "                  ...   \n",
       "parado          0.000200\n",
       "montadora       0.000200\n",
       "cv0ohmpumh      0.000200\n",
       "vbxefpqkkh      0.000200\n",
       "more            0.000200\n",
       "velocity        0.000200\n",
       "sugado          0.000200\n",
       "duda_bri        0.000200\n",
       "percebi         0.000200\n",
       "pet             0.000200\n",
       "enjoar          0.000200\n",
       "conselho        0.000200\n",
       "grupo           0.000200\n",
       "diÃ¡riomotor     0.000200\n",
       "euros           0.000200\n",
       "browniecarro    0.000200\n",
       "vira            0.000200\n",
       "mrohdwwf4w      0.000200\n",
       "existiam        0.000200\n",
       "prontos         0.000200\n",
       "barca           0.000200\n",
       "olocooo         0.000200\n",
       "acordando       0.000200\n",
       "process         0.000200\n",
       "hq5l2ra3jj      0.000200\n",
       "velhos          0.000200\n",
       "trabalho        0.000200\n",
       "ruicouto7311    0.000200\n",
       "ðŸ¾â€              0.000200\n",
       "yagnvcmbnh      0.000200\n",
       "Length: 1676, dtype: float64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras_universo_treino.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bugatti           0.051213\n",
       "o                 0.037062\n",
       "a                 0.035714\n",
       "de                0.035040\n",
       "chiron            0.030323\n",
       "km/h              0.025606\n",
       "que               0.020889\n",
       "490               0.020889\n",
       "carro             0.018194\n",
       "um                0.017520\n",
       "do                0.015499\n",
       "em                0.014825\n",
       "velocidade        0.013477\n",
       "mais              0.013477\n",
       "recorde           0.012803\n",
       "e                 0.011456\n",
       "Ã©                 0.010108\n",
       "com               0.010108\n",
       "nÃ£o               0.009434\n",
       "da                0.008760\n",
       "300               0.007412\n",
       "mph               0.006739\n",
       "sport             0.006739\n",
       "os                0.006065\n",
       "super             0.005391\n",
       "chega             0.005391\n",
       "mundial           0.005391\n",
       "para              0.004717\n",
       "por               0.004717\n",
       "eu                0.004717\n",
       "                    ...   \n",
       "pergunto          0.000674\n",
       "coro              0.000674\n",
       "alguÃ©m            0.000674\n",
       "conhecidos        0.000674\n",
       "tÃ­tulo            0.000674\n",
       "percam            0.000674\n",
       "h6skmuy1ow        0.000674\n",
       "demitir           0.000674\n",
       "cv0ohmpumh        0.000674\n",
       "bonitos           0.000674\n",
       "ntipvegqfw        0.000674\n",
       "esmagar           0.000674\n",
       "absurdamente      0.000674\n",
       "primeira          0.000674\n",
       "estrondosos       0.000674\n",
       "estÃ¡              0.000674\n",
       "lamborghiniatÃ©    0.000674\n",
       "pauloramos1       0.000674\n",
       "homologado        0.000674\n",
       "porsche           0.000674\n",
       "rocket            0.000674\n",
       "helicÃ³ptero       0.000674\n",
       "falando           0.000674\n",
       "tÃªm               0.000674\n",
       "sou               0.000674\n",
       "conceito          0.000674\n",
       "d6jjf0unes        0.000674\n",
       "Ã s                0.000674\n",
       "lbwtnqyh07        0.000674\n",
       "wow               0.000674\n",
       "Length: 522, dtype: float64"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras_relevantes_treino.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bugatti         0.041690\n",
       "de              0.021554\n",
       "a               0.020136\n",
       "o               0.018151\n",
       "e               0.014748\n",
       "na              0.013897\n",
       "nÃ£o             0.013613\n",
       "um              0.013613\n",
       "do              0.012479\n",
       "que             0.012479\n",
       "nome            0.011061\n",
       "primeira        0.009075\n",
       "eu              0.009075\n",
       "seu             0.008225\n",
       "letra           0.007374\n",
       "gente           0.007090\n",
       "Ã©               0.006807\n",
       "bananaobjeto    0.006523\n",
       "vamos           0.006239\n",
       "vale            0.006239\n",
       "jogar           0.006239\n",
       "com             0.006239\n",
       "olhar           0.006239\n",
       "respondanome    0.005956\n",
       "stop            0.005956\n",
       "brancofruta     0.005956\n",
       "bugatticor      0.005672\n",
       "chiron          0.005672\n",
       "para            0.005389\n",
       "carro           0.005389\n",
       "                  ...   \n",
       "amo             0.000284\n",
       "pensar          0.000284\n",
       "quatrocentos    0.000284\n",
       "itupeva         0.000284\n",
       "mateca          0.000284\n",
       "fxnqayanji      0.000284\n",
       "bolsa           0.000284\n",
       "escolham        0.000284\n",
       "scglfo0s1p      0.000284\n",
       "quebrando       0.000284\n",
       "adesivado       0.000284\n",
       "fefuybnt3l      0.000284\n",
       "aberto          0.000284\n",
       "flagrado        0.000284\n",
       "colocar         0.000284\n",
       "tubarÃ£o         0.000284\n",
       "siiiiim         0.000284\n",
       "produto         0.000284\n",
       "thecrew2â€¦       0.000284\n",
       "407             0.000284\n",
       "aparecer        0.000284\n",
       "pzzminujgo      0.000284\n",
       "b               0.000284\n",
       "mantendo        0.000284\n",
       "big             0.000284\n",
       "palestra        0.000284\n",
       "feliz           0.000284\n",
       "semÃ¡foro        0.000284\n",
       "le              0.000284\n",
       "3tzw5cfyey      0.000284\n",
       "Length: 1341, dtype: float64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras_nao_relevantes_treino.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2962075848303393"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_relevante = palavras_relevantes.size/universo.size\n",
    "P_relevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7037924151696606"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_nao_relevante = palavras_nao_relevantes.size/universo.size\n",
    "P_nao_relevante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora vocÃª deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2 = pd.read_excel('Bugatti_teste.xlsx')\n",
    "relevante_teste_raw2 = raw2[raw2['RelevÃ¢ncia'] == 1]\n",
    "nao_relevante_teste_raw2 = raw2[raw2[\"RelevÃ¢ncia\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista1 = []\n",
    "for tweet in relevante_teste_raw2['Teste']:\n",
    "    texto = transform_emojis(tweet)\n",
    "    lista1.append(cleanup(texto))\n",
    "    \n",
    "tweets_relevante_teste = pd.Series(lista1)\n",
    "tweets_relevante_teste = tweets_relevante_teste.str.replace('|'.join(l),'',regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista2 = []\n",
    "for tweet in nao_relevante_teste_raw2['Teste']:\n",
    "    texto = transform_emojis(tweet)\n",
    "    lista2.append(cleanup(texto))\n",
    "    \n",
    "tweets_nao_relevante_teste = pd.Series(lista2)\n",
    "tweets_nao_relevante_teste = tweets_nao_relevante_teste.str.replace('|'.join(l),'',regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista3 = []\n",
    "for tweet in raw2['Teste']:\n",
    "    texto = transform_emojis(tweet)\n",
    "    lista3.append(cleanup(texto))\n",
    "    \n",
    "tweets_universo_teste = pd.Series(lista3)\n",
    "tweets_universo_teste = tweets_universo_teste.str.replace('|'.join(l),'',regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bugatti           137\n",
       "a                  92\n",
       "de                 73\n",
       "o                  66\n",
       "e                  63\n",
       "do                 51\n",
       "que                51\n",
       "chiron             47\n",
       "nÃ£o                43\n",
       "km/h               42\n",
       "um                 39\n",
       "Ã©                  37\n",
       "490                32\n",
       "com                31\n",
       "carro              31\n",
       "recorde            28\n",
       "na                 26\n",
       "uma                23\n",
       "nome               21\n",
       "eu                 21\n",
       "em                 20\n",
       "mais               19\n",
       "primeira           18\n",
       "seu                18\n",
       "pra                17\n",
       "300                17\n",
       "bananaobjeto       17\n",
       "no                 17\n",
       "vamos              16\n",
       "bugatticor         16\n",
       "                 ... \n",
       "jockey              1\n",
       "publico             1\n",
       "mangoldmikel        1\n",
       "franquia            1\n",
       "revolucionou        1\n",
       "bolocarro           1\n",
       "bananacarro         1\n",
       "capÃ­tulos           1\n",
       "influenciou         1\n",
       "deu_tigre2Â°         1\n",
       "tweet               1\n",
       "11384e              1\n",
       "lembrando           1\n",
       "lego                1\n",
       "mule                1\n",
       "â€œcivilâ€             1\n",
       "ano                 1\n",
       "mateca              1\n",
       "balÃ£oanimal         1\n",
       "leis                1\n",
       "6305                1\n",
       "170/180km/h         1\n",
       "bixao               1\n",
       "importadora         1\n",
       "ðŸ¼â€                  1\n",
       "yebmfxloh8          1\n",
       "ferdinand           1\n",
       "pronto              1\n",
       "spirosmargaris      1\n",
       "desse               1\n",
       "Length: 1283, dtype: int64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista4 = []\n",
    "for tweet in tweets_universo_teste:\n",
    "    lista4 = lista4 + tweet.split()\n",
    "\n",
    "palavras_universo_teste = pd.Series(lista4)\n",
    "palavras_universo_teste.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista5 = []\n",
    "for tweet in tweets_relevante_teste:\n",
    "    lista5 = lista5 + tweet.split()\n",
    "\n",
    "palavras_relevantes_teste = pd.Series(lista5)\n",
    "freq_rel_palavras_relevantes_teste = palavras_relevantes_teste.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bugatti            0.055959\n",
       "a                  0.039971\n",
       "chiron             0.030523\n",
       "o                  0.028343\n",
       "km/h               0.027616\n",
       "de                 0.023983\n",
       "490                0.023256\n",
       "e                  0.021076\n",
       "recorde            0.020349\n",
       "que                0.015262\n",
       "do                 0.013808\n",
       "um                 0.013808\n",
       "carro              0.013081\n",
       "Ã©                  0.012355\n",
       "velocidade         0.011628\n",
       "300                0.010901\n",
       "em                 0.010174\n",
       "chega              0.010174\n",
       "com                0.010174\n",
       "quebra             0.007267\n",
       "mais               0.007267\n",
       "nÃ£o                0.007267\n",
       "astonished         0.007267\n",
       "novo               0.006541\n",
       "super              0.006541\n",
       "mundial            0.006541\n",
       "mph                0.006541\n",
       "aos                0.005814\n",
       "vÃ­deo              0.005814\n",
       "via                0.005814\n",
       "                     ...   \n",
       "anuncieseucarro    0.000727\n",
       "constantemente     0.000727\n",
       "uniu               0.000727\n",
       "Ã©este              0.000727\n",
       "trazer             0.000727\n",
       "76                 0.000727\n",
       "pronto             0.000727\n",
       "caralhooo          0.000727\n",
       "Ã                   0.000727\n",
       "nada               0.000727\n",
       "prÃ³prios           0.000727\n",
       "demais             0.000727\n",
       "levemente          0.000727\n",
       "p                  0.000727\n",
       "dy29z8fexg         0.000727\n",
       "yebmfxloh8         0.000727\n",
       "deixou             0.000727\n",
       "faz                0.000727\n",
       "jrqtctfgqs         0.000727\n",
       "certeza            0.000727\n",
       "dessa              0.000727\n",
       "16aov5h7px         0.000727\n",
       "automation         0.000727\n",
       "Ãºltimos            0.000727\n",
       "pessoal            0.000727\n",
       "andar              0.000727\n",
       "ao                 0.000727\n",
       "kkkkkkkk           0.000727\n",
       "rnnnlzk8lj         0.000727\n",
       "vhpz56hawv         0.000727\n",
       "Length: 521, dtype: float64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_rel_palavras_relevantes_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lista6 = []\n",
    "for tweet in tweets_nao_relevante_teste:\n",
    "    lista6 = lista6 + tweet.split()\n",
    "\n",
    "palavras_nao_relevantes_teste = pd.Series(lista6)\n",
    "freq_rel_palavras_nao_relevantes_teste = palavras_nao_relevantes_teste.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bugatti                                                      0.029412\n",
       "de                                                           0.019608\n",
       "a                                                            0.018137\n",
       "e                                                            0.016667\n",
       "nÃ£o                                                          0.016176\n",
       "do                                                           0.015686\n",
       "que                                                          0.014706\n",
       "o                                                            0.013235\n",
       "na                                                           0.011765\n",
       "uma                                                          0.009804\n",
       "um                                                           0.009804\n",
       "nome                                                         0.009804\n",
       "Ã©                                                            0.009804\n",
       "eu                                                           0.008333\n",
       "primeira                                                     0.008333\n",
       "bananaobjeto                                                 0.008333\n",
       "com                                                          0.008333\n",
       "seu                                                          0.007843\n",
       "bugatticor                                                   0.007843\n",
       "vamos                                                        0.007353\n",
       "letra                                                        0.007353\n",
       "pra                                                          0.007353\n",
       "olhar                                                        0.006863\n",
       "jogar                                                        0.006863\n",
       "gente                                                        0.006863\n",
       "respondanome                                                 0.006863\n",
       "vale                                                         0.006863\n",
       "stop                                                         0.006863\n",
       "brancofruta                                                  0.006373\n",
       "no                                                           0.006373\n",
       "                                                               ...   \n",
       "objeto                                                       0.000490\n",
       "outro                                                        0.000490\n",
       "chegando                                                     0.000490\n",
       "ver                                                          0.000490\n",
       "breno                                                        0.000490\n",
       "polÃ­cia                                                      0.000490\n",
       "tomar                                                        0.000490\n",
       "porshe                                                       0.000490\n",
       "birra                                                        0.000490\n",
       "vermelhobebida                                               0.000490\n",
       "vapeboy                                                      0.000490\n",
       "Ã´nibus                                                       0.000490\n",
       "vÃ©i                                                          0.000490\n",
       "bÃ¡rbaraberlimbourbonbeijinhobugattibegebananabatutabarata    0.000490\n",
       "ofereci                                                      0.000490\n",
       "blindado                                                     0.000490\n",
       "veyront                                                      0.000490\n",
       "precisa                                                      0.000490\n",
       "turbo                                                        0.000490\n",
       "noteikkkkkkkkkkkk                                            0.000490\n",
       "venceu                                                       0.000490\n",
       "dai                                                          0.000490\n",
       "bacalhau                                                     0.000490\n",
       "bolonhesacarro                                               0.000490\n",
       "quero                                                        0.000490\n",
       "9685                                                         0.000490\n",
       "tiro                                                         0.000490\n",
       "lhes                                                         0.000490\n",
       "fbxlbsdt6b                                                   0.000490\n",
       "palace                                                       0.000490\n",
       "Length: 892, dtype: float64"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_rel_palavras_nao_relevantes_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montando o Classificardor Naive Bayes\n",
    "\n",
    "#### Probabilidades de ser relevante ou nÃ£o relevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3416"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_palavras_teste = palavras_relevantes_teste.size + palavras_nao_relevantes_teste.size\n",
    "num_palavras_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probabilidade do tweet ser relevante\n",
    "prob_relevante_teste = tweets_relevante_teste.size / tweets_universo_teste.size\n",
    "prob_relevante_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probabilidade do tweet nÃ£o ser relevante\n",
    "prob_nao_relevante_teste = tweets_nao_relevante_teste.size / tweets_universo_teste.size\n",
    "prob_nao_relevante_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como o classificador funcionarÃ¡:\n",
    "\n",
    "Todos os tweets presentes na planilha de testes serÃ£o, individualmente, analisados pelo classificador e catalogados, em um loop. Depois de todo esse processo, os resultados de cada tweet analisado, que podem ser ou \"Relevante\" ou \"Irrelevante\", pelo menos neste primeiro momento, serÃ£o adicionados a uma lista com o fim de serem comparados com a anÃ¡lise feita Ã  mÃ£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef Bayes(relevancia,serie1,serie2):\\n    #Criando o loop para o cÃ¡lculo da probabilidade de ser relevante:\\n    i = 0 \\n    lista_resultados_numericos = []\\n\\n    #selecionando o tweet\\n    for tweets in testudo:\\n        \\n        #transformando em lista de str\\n        tweetr = tweets.split()\\n        \\n        # Calculando a probabilidade deste tweet ser relevante dado as suas palavras\\n        log_probabilidade = 0\\n        for palavra in tweetr:\\n        \\n            #NÃ£o deixa que palavras nÃ£o pertencentes Ã  sÃ©rie bugem o cÃ³digo\\n            if palavra not in serie1:\\n                log_probabilidade += math.log(1/3416, 10)\\n            else:\\n                log_probabilidade += math.log(serie2[palavra] + 1/3416, 10)\\n        \\n        #Guardando os resultados e resetando a variÃ¡vel probabilidade    \\n        lista_resultados_numericos.append(log_probabilidade + math.log(relevancia, 10))\\n        log_probabilidade = 0\\n        i += 1\\n\\n    return lista_resultados_numerico'\\n\""
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def Bayes(relevancia,serie1,serie2):\n",
    "    #Criando o loop para o cÃ¡lculo da probabilidade de ser relevante:\n",
    "    i = 0 \n",
    "    lista_resultados_numericos = []\n",
    "\n",
    "    #selecionando o tweet\n",
    "    for tweets in testudo:\n",
    "        \n",
    "        #transformando em lista de str\n",
    "        tweetr = tweets.split()\n",
    "        \n",
    "        # Calculando a probabilidade deste tweet ser relevante dado as suas palavras\n",
    "        log_probabilidade = 0\n",
    "        for palavra in tweetr:\n",
    "        \n",
    "            #NÃ£o deixa que palavras nÃ£o pertencentes Ã  sÃ©rie bugem o cÃ³digo\n",
    "            if palavra not in serie1:\n",
    "                log_probabilidade += math.log(1/3416, 10)\n",
    "            else:\n",
    "                log_probabilidade += math.log(serie2[palavra] + 1/3416, 10)\n",
    "        \n",
    "        #Guardando os resultados e resetando a variÃ¡vel probabilidade    \n",
    "        lista_resultados_numericos.append(log_probabilidade + math.log(relevancia, 10))\n",
    "        log_probabilidade = 0\n",
    "        i += 1\n",
    "\n",
    "    return lista_resultados_numerico'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1e-10 #Ã© uma estimativa e deve mudar\n",
    "vocabulario = 356000\n",
    "\n",
    "# defini probabilidade de ser relevante:\n",
    "def relevante(tweet):\n",
    "    \n",
    "    tweet_relevante_4 = tweet.split()\n",
    "\n",
    "    prob = 1\n",
    "    \n",
    "    for p in tweet_relevante_4:\n",
    "        \n",
    "        if p in freq_rel_palavras_relevantes_teste:\n",
    "            prob = prob*(freq_rel_palavras_relevantes_teste[p]*palavras_relevantes_teste.size+smooth)/(palavras_relevantes_teste.size+smooth*vocabulario)*1000\n",
    "        else:\n",
    "            prob = prob*(smooth)/(palavras_relevantes_teste.size+smooth*vocabulario)*1000\n",
    "            \n",
    "    prob = prob * prob_relevante_teste\n",
    "    prob = math.log10(prob)\n",
    "    #print(prob)\n",
    "    return prob\n",
    "\n",
    "# defini probabilidade de ser irrelevante:\n",
    "def irrelevante(tweet):\n",
    "    \n",
    "    tweet_irrelevante_4 = tweet.split()\n",
    "            \n",
    "    prob = 1\n",
    "    \n",
    "    for p in tweet_irrelevante_4:\n",
    "        \n",
    "        if p in freq_rel_palavras_nao_relevantes_teste:\n",
    "            prob = prob*(freq_rel_palavras_nao_relevantes_teste[p]*palavras_nao_relevantes_teste.size+smooth)/(palavras_nao_relevantes_teste.size+smooth*vocabulario)*1000\n",
    "        else:\n",
    "            prob = prob*(smooth)/(palavras_nao_relevantes_teste.size+smooth*vocabulario)*1000\n",
    "            \n",
    "    prob = prob * prob_nao_relevante_teste\n",
    "    prob = math.log10(prob)\n",
    "    #print(prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "lista_resultados = []\n",
    "for tweet in tweets_universo_teste:\n",
    "    \n",
    "    if relevante(tweet) > irrelevante(tweet):\n",
    "           lista_resultados.append(1)\n",
    "            \n",
    "    if relevante(tweet) < irrelevante(tweet):\n",
    "        lista_resultados.append(0)\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "lista_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "0\n",
      "124\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "verdadeiros_positivos = 0\n",
    "falsos_positivos = 0\n",
    "verdadeiros_negativos = 0\n",
    "falsos_negativos = 0\n",
    "\n",
    "for tweet in raw2['RelevÃ¢ncia']:\n",
    "    if tweet == lista_resultados[i] and lista_resultados[i] == 1:\n",
    "        verdadeiros_positivos += 1\n",
    "    \n",
    "    if tweet == lista_resultados[i] and lista_resultados[i] == 0:\n",
    "        verdadeiros_negativos += 1\n",
    "    \n",
    "    if tweet != lista_resultados[i] and lista_resultados[i] == 1:\n",
    "        falso_positivos += 1\n",
    "        \n",
    "    if tweet != lista_resultados[i] and lista_resultados[i] == 0:\n",
    "        falsos_negativos += 1\n",
    "        \n",
    "    i += 1\n",
    "        \n",
    "print(verdadeiros_positivos)\n",
    "print(falsos_positivos)\n",
    "print(verdadeiros_negativos)\n",
    "print(falsos_negativos)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AperfeiÃ§oamento:\n",
    "\n",
    "Os trabalhos vÃ£o evoluir em conceito dependendo da quantidade de itens avanÃ§ados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separaÃ§Ã£o de espaÃ§os entre palavras e emojis ou emojis e emojis\n",
    "* Propor outras limpezas e transformaÃ§Ãµes que nÃ£o afetem a qualidade da informaÃ§Ã£o ou classificaÃ§Ã£o\n",
    "* Criar categorias intermediÃ¡rias de relevÃ¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que nÃ£o posso usar o prÃ³prio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenÃ¡rios para NaÃ¯ve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicaÃ§Ãµes concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza anÃ¡lise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReferÃªncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
