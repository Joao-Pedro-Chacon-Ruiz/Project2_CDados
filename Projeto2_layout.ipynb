{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nome: Breno Marti**\n",
    "\n",
    "**Nome: João Pedro Chacon Ruiz**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador automático de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "!pip install emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta: ***[Preencha aqui o id da sua conta. Ex: @fulano ]***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Dados de autenticação do twitter:\\n\\n#Coloque aqui o identificador da conta no twitter: @fulano\\n\\n#leitura do arquivo no formato JSON\\nwith open('auth.pass') as fp:    \\n    data = json.load(fp)\\n\\n#Configurando a biblioteca. Não modificar\\nauth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\\nauth.set_access_token(data['access_token'], data['access_token_secret'])\\n\""
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @fulano\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas do projeto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de um produto e coleta das mensagens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'Bugatti'\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Cria um objeto para a captura\\napi = tweepy.API(auth)\\n\\n#Inicia a captura, para mais detalhes: ver a documentação do tweepy\\ni = 1\\nmsgs = []\\nfor msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode=\"extended\").items():\\n    if (not msg.retweeted) and (\\'RT\\' not in msg.full_text):\\n        msgs.append(msg.full_text.lower())\\n        i += 1\\n    if i > n:\\n        break\\n\\n#Embaralhando as mensagens para reduzir um possível viés\\nshuffle(msgs\\'\\n'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode=\"extended\").items():\n",
    "    if (not msg.retweeted) and ('RT' not in msg.full_text):\n",
    "        msgs.append(msg.full_text.lower())\n",
    "        i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Verifica se o arquivo não existe para não substituir um conjunto pronto\\nif not os.path.isfile('./{0}.xlsx'.format(produto)):\\n    \\n    #Abre o arquivo para escrita\\n    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\\n\\n    #divide o conjunto de mensagens em duas planilhas\\n    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\\n    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\\n\\n    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\\n    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\\n\\n    #fecha o arquivo\\n    writer.save()\\n\""
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Classificando as mensagens na coragem\n",
    "\n",
    "Esta etapa é manual. Faça a mesma pelo Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_excel('Bugatti_treino.xlsx')\n",
    "relevante_treino_raw = raw[raw['Relevância'] == 1]\n",
    "nao_relevante_treino_raw = raw[raw[\"Relevância\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções de limpeza de texto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Função de limpeza muito simples que troca alguns sinais básicos por espaços\n",
    "    \"\"\"\n",
    "    punctuation = '[#@,!\\()-.€\":?;]' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    return text_subbed   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_emojis(text):\n",
    "    clean = emojis.decode(text)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['\\n','https','//t','co/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpando e transformando os dataframes em séries para a análise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "for tweet in relevante_treino_raw['Treinamento']:\n",
    "    texto = transform_emojis(tweet)\n",
    "    list1.append(cleanup(texto))\n",
    "    \n",
    "relevante_treino = pd.Series(list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota do professor: explicar como o código abaixo funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_relevante_treino = relevante_treino.str.replace('|'.join(l),'',regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = []\n",
    "for tweet in nao_relevante_treino_raw['Treinamento']:\n",
    "    texto = transform_emojis(tweet)\n",
    "    list2.append(cleanup(texto))\n",
    "    \n",
    "nao_relevante_treino = pd.Series(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_nao_relevante_treino = nao_relevante_treino.str.replace('|'.join(l),'',regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "list3 = []\n",
    "for tweet in raw['Treinamento']:\n",
    "    texto = transform_emojis(tweet)\n",
    "    list3.append(cleanup(texto))\n",
    "    \n",
    "tudo = pd.Series(list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_treino = tudo.str.replace('|'.join(l),'',regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contando o número de aparições de cada palavra :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list4 = []\n",
    "for tweet in tweets_treino:\n",
    "    list4 = list4 + tweet.split()\n",
    "\n",
    "palavras_universo_treino = pd.Series(list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list5 = []\n",
    "for tweet in tweets_relevante_treino:\n",
    "    list5 = list5 + tweet.split()\n",
    "\n",
    "palavras_relevantes_treino = pd.Series(list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list6 = []\n",
    "for tweet in tweets_nao_relevante_treino:\n",
    "    list6 = list6 + tweet.split()\n",
    "\n",
    "palavras_nao_relevantes_treino = pd.Series(list6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequência relativa de cada palavra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bugatti         0.044511\n",
       "de              0.025549\n",
       "a               0.024750\n",
       "o               0.023752\n",
       "que             0.014970\n",
       "um              0.014770\n",
       "e               0.013772\n",
       "do              0.013373\n",
       "chiron          0.012974\n",
       "não             0.012375\n",
       "na              0.010978\n",
       "carro           0.009182\n",
       "km/h            0.009182\n",
       "nome            0.007984\n",
       "490             0.007984\n",
       "eu              0.007784\n",
       "é               0.007784\n",
       "com             0.007385\n",
       "em              0.006986\n",
       "primeira        0.006587\n",
       "mais            0.006587\n",
       "seu             0.005988\n",
       "gente           0.005190\n",
       "letra           0.005190\n",
       "para            0.005190\n",
       "por             0.004990\n",
       "velocidade      0.004790\n",
       "da              0.004790\n",
       "recorde         0.004591\n",
       "bananaobjeto    0.004591\n",
       "                  ...   \n",
       "parado          0.000200\n",
       "montadora       0.000200\n",
       "cv0ohmpumh      0.000200\n",
       "vbxefpqkkh      0.000200\n",
       "more            0.000200\n",
       "velocity        0.000200\n",
       "sugado          0.000200\n",
       "duda_bri        0.000200\n",
       "percebi         0.000200\n",
       "pet             0.000200\n",
       "enjoar          0.000200\n",
       "conselho        0.000200\n",
       "grupo           0.000200\n",
       "diáriomotor     0.000200\n",
       "euros           0.000200\n",
       "browniecarro    0.000200\n",
       "vira            0.000200\n",
       "mrohdwwf4w      0.000200\n",
       "existiam        0.000200\n",
       "prontos         0.000200\n",
       "barca           0.000200\n",
       "olocooo         0.000200\n",
       "acordando       0.000200\n",
       "process         0.000200\n",
       "hq5l2ra3jj      0.000200\n",
       "velhos          0.000200\n",
       "trabalho        0.000200\n",
       "ruicouto7311    0.000200\n",
       "🏾‍              0.000200\n",
       "yagnvcmbnh      0.000200\n",
       "Length: 1676, dtype: float64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras_universo_treino.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bugatti           0.051213\n",
       "o                 0.037062\n",
       "a                 0.035714\n",
       "de                0.035040\n",
       "chiron            0.030323\n",
       "km/h              0.025606\n",
       "que               0.020889\n",
       "490               0.020889\n",
       "carro             0.018194\n",
       "um                0.017520\n",
       "do                0.015499\n",
       "em                0.014825\n",
       "velocidade        0.013477\n",
       "mais              0.013477\n",
       "recorde           0.012803\n",
       "e                 0.011456\n",
       "é                 0.010108\n",
       "com               0.010108\n",
       "não               0.009434\n",
       "da                0.008760\n",
       "300               0.007412\n",
       "mph               0.006739\n",
       "sport             0.006739\n",
       "os                0.006065\n",
       "super             0.005391\n",
       "chega             0.005391\n",
       "mundial           0.005391\n",
       "para              0.004717\n",
       "por               0.004717\n",
       "eu                0.004717\n",
       "                    ...   \n",
       "pergunto          0.000674\n",
       "coro              0.000674\n",
       "alguém            0.000674\n",
       "conhecidos        0.000674\n",
       "título            0.000674\n",
       "percam            0.000674\n",
       "h6skmuy1ow        0.000674\n",
       "demitir           0.000674\n",
       "cv0ohmpumh        0.000674\n",
       "bonitos           0.000674\n",
       "ntipvegqfw        0.000674\n",
       "esmagar           0.000674\n",
       "absurdamente      0.000674\n",
       "primeira          0.000674\n",
       "estrondosos       0.000674\n",
       "está              0.000674\n",
       "lamborghiniaté    0.000674\n",
       "pauloramos1       0.000674\n",
       "homologado        0.000674\n",
       "porsche           0.000674\n",
       "rocket            0.000674\n",
       "helicóptero       0.000674\n",
       "falando           0.000674\n",
       "têm               0.000674\n",
       "sou               0.000674\n",
       "conceito          0.000674\n",
       "d6jjf0unes        0.000674\n",
       "às                0.000674\n",
       "lbwtnqyh07        0.000674\n",
       "wow               0.000674\n",
       "Length: 522, dtype: float64"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras_relevantes_treino.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bugatti         0.041690\n",
       "de              0.021554\n",
       "a               0.020136\n",
       "o               0.018151\n",
       "e               0.014748\n",
       "na              0.013897\n",
       "não             0.013613\n",
       "um              0.013613\n",
       "do              0.012479\n",
       "que             0.012479\n",
       "nome            0.011061\n",
       "primeira        0.009075\n",
       "eu              0.009075\n",
       "seu             0.008225\n",
       "letra           0.007374\n",
       "gente           0.007090\n",
       "é               0.006807\n",
       "bananaobjeto    0.006523\n",
       "vamos           0.006239\n",
       "vale            0.006239\n",
       "jogar           0.006239\n",
       "com             0.006239\n",
       "olhar           0.006239\n",
       "respondanome    0.005956\n",
       "stop            0.005956\n",
       "brancofruta     0.005956\n",
       "bugatticor      0.005672\n",
       "chiron          0.005672\n",
       "para            0.005389\n",
       "carro           0.005389\n",
       "                  ...   \n",
       "amo             0.000284\n",
       "pensar          0.000284\n",
       "quatrocentos    0.000284\n",
       "itupeva         0.000284\n",
       "mateca          0.000284\n",
       "fxnqayanji      0.000284\n",
       "bolsa           0.000284\n",
       "escolham        0.000284\n",
       "scglfo0s1p      0.000284\n",
       "quebrando       0.000284\n",
       "adesivado       0.000284\n",
       "fefuybnt3l      0.000284\n",
       "aberto          0.000284\n",
       "flagrado        0.000284\n",
       "colocar         0.000284\n",
       "tubarão         0.000284\n",
       "siiiiim         0.000284\n",
       "produto         0.000284\n",
       "thecrew2…       0.000284\n",
       "407             0.000284\n",
       "aparecer        0.000284\n",
       "pzzminujgo      0.000284\n",
       "b               0.000284\n",
       "mantendo        0.000284\n",
       "big             0.000284\n",
       "palestra        0.000284\n",
       "feliz           0.000284\n",
       "semáforo        0.000284\n",
       "le              0.000284\n",
       "3tzw5cfyey      0.000284\n",
       "Length: 1341, dtype: float64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras_nao_relevantes_treino.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2962075848303393"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_relevante = palavras_relevantes.size/universo.size\n",
    "P_relevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7037924151696606"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_nao_relevante = palavras_nao_relevantes.size/universo.size\n",
    "P_nao_relevante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2 = pd.read_excel('Bugatti_teste.xlsx')\n",
    "relevante_teste_raw2 = raw2[raw2['Relevância'] == 1]\n",
    "nao_relevante_teste_raw2 = raw2[raw2[\"Relevância\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista1 = []\n",
    "for tweet in relevante_teste_raw2['Teste']:\n",
    "    texto = transform_emojis(tweet)\n",
    "    lista1.append(cleanup(texto))\n",
    "    \n",
    "tweets_relevante_teste = pd.Series(lista1)\n",
    "tweets_relevante_teste = tweets_relevante_teste.str.replace('|'.join(l),'',regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista2 = []\n",
    "for tweet in nao_relevante_teste_raw2['Teste']:\n",
    "    texto = transform_emojis(tweet)\n",
    "    lista2.append(cleanup(texto))\n",
    "    \n",
    "tweets_nao_relevante_teste = pd.Series(lista2)\n",
    "tweets_nao_relevante_teste = tweets_nao_relevante_teste.str.replace('|'.join(l),'',regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista3 = []\n",
    "for tweet in raw2['Teste']:\n",
    "    texto = transform_emojis(tweet)\n",
    "    lista3.append(cleanup(texto))\n",
    "    \n",
    "tweets_universo_teste = pd.Series(lista3)\n",
    "tweets_universo_teste = tweets_universo_teste.str.replace('|'.join(l),'',regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bugatti           137\n",
       "a                  92\n",
       "de                 73\n",
       "o                  66\n",
       "e                  63\n",
       "do                 51\n",
       "que                51\n",
       "chiron             47\n",
       "não                43\n",
       "km/h               42\n",
       "um                 39\n",
       "é                  37\n",
       "490                32\n",
       "com                31\n",
       "carro              31\n",
       "recorde            28\n",
       "na                 26\n",
       "uma                23\n",
       "nome               21\n",
       "eu                 21\n",
       "em                 20\n",
       "mais               19\n",
       "primeira           18\n",
       "seu                18\n",
       "pra                17\n",
       "300                17\n",
       "bananaobjeto       17\n",
       "no                 17\n",
       "vamos              16\n",
       "bugatticor         16\n",
       "                 ... \n",
       "jockey              1\n",
       "publico             1\n",
       "mangoldmikel        1\n",
       "franquia            1\n",
       "revolucionou        1\n",
       "bolocarro           1\n",
       "bananacarro         1\n",
       "capítulos           1\n",
       "influenciou         1\n",
       "deu_tigre2°         1\n",
       "tweet               1\n",
       "11384e              1\n",
       "lembrando           1\n",
       "lego                1\n",
       "mule                1\n",
       "“civil”             1\n",
       "ano                 1\n",
       "mateca              1\n",
       "balãoanimal         1\n",
       "leis                1\n",
       "6305                1\n",
       "170/180km/h         1\n",
       "bixao               1\n",
       "importadora         1\n",
       "🏼‍                  1\n",
       "yebmfxloh8          1\n",
       "ferdinand           1\n",
       "pronto              1\n",
       "spirosmargaris      1\n",
       "desse               1\n",
       "Length: 1283, dtype: int64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista4 = []\n",
    "for tweet in tweets_universo_teste:\n",
    "    lista4 = lista4 + tweet.split()\n",
    "\n",
    "palavras_universo_teste = pd.Series(lista4)\n",
    "palavras_universo_teste.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista5 = []\n",
    "for tweet in tweets_relevante_teste:\n",
    "    lista5 = lista5 + tweet.split()\n",
    "\n",
    "palavras_relevantes_teste = pd.Series(lista5)\n",
    "freq_rel_palavras_relevantes_teste = palavras_relevantes_teste.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bugatti            0.055959\n",
       "a                  0.039971\n",
       "chiron             0.030523\n",
       "o                  0.028343\n",
       "km/h               0.027616\n",
       "de                 0.023983\n",
       "490                0.023256\n",
       "e                  0.021076\n",
       "recorde            0.020349\n",
       "que                0.015262\n",
       "do                 0.013808\n",
       "um                 0.013808\n",
       "carro              0.013081\n",
       "é                  0.012355\n",
       "velocidade         0.011628\n",
       "300                0.010901\n",
       "em                 0.010174\n",
       "chega              0.010174\n",
       "com                0.010174\n",
       "quebra             0.007267\n",
       "mais               0.007267\n",
       "não                0.007267\n",
       "astonished         0.007267\n",
       "novo               0.006541\n",
       "super              0.006541\n",
       "mundial            0.006541\n",
       "mph                0.006541\n",
       "aos                0.005814\n",
       "vídeo              0.005814\n",
       "via                0.005814\n",
       "                     ...   \n",
       "anuncieseucarro    0.000727\n",
       "constantemente     0.000727\n",
       "uniu               0.000727\n",
       "éeste              0.000727\n",
       "trazer             0.000727\n",
       "76                 0.000727\n",
       "pronto             0.000727\n",
       "caralhooo          0.000727\n",
       "à                  0.000727\n",
       "nada               0.000727\n",
       "próprios           0.000727\n",
       "demais             0.000727\n",
       "levemente          0.000727\n",
       "p                  0.000727\n",
       "dy29z8fexg         0.000727\n",
       "yebmfxloh8         0.000727\n",
       "deixou             0.000727\n",
       "faz                0.000727\n",
       "jrqtctfgqs         0.000727\n",
       "certeza            0.000727\n",
       "dessa              0.000727\n",
       "16aov5h7px         0.000727\n",
       "automation         0.000727\n",
       "últimos            0.000727\n",
       "pessoal            0.000727\n",
       "andar              0.000727\n",
       "ao                 0.000727\n",
       "kkkkkkkk           0.000727\n",
       "rnnnlzk8lj         0.000727\n",
       "vhpz56hawv         0.000727\n",
       "Length: 521, dtype: float64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_rel_palavras_relevantes_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lista6 = []\n",
    "for tweet in tweets_nao_relevante_teste:\n",
    "    lista6 = lista6 + tweet.split()\n",
    "\n",
    "palavras_nao_relevantes_teste = pd.Series(lista6)\n",
    "freq_rel_palavras_nao_relevantes_teste = palavras_nao_relevantes_teste.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bugatti                                                      0.029412\n",
       "de                                                           0.019608\n",
       "a                                                            0.018137\n",
       "e                                                            0.016667\n",
       "não                                                          0.016176\n",
       "do                                                           0.015686\n",
       "que                                                          0.014706\n",
       "o                                                            0.013235\n",
       "na                                                           0.011765\n",
       "uma                                                          0.009804\n",
       "um                                                           0.009804\n",
       "nome                                                         0.009804\n",
       "é                                                            0.009804\n",
       "eu                                                           0.008333\n",
       "primeira                                                     0.008333\n",
       "bananaobjeto                                                 0.008333\n",
       "com                                                          0.008333\n",
       "seu                                                          0.007843\n",
       "bugatticor                                                   0.007843\n",
       "vamos                                                        0.007353\n",
       "letra                                                        0.007353\n",
       "pra                                                          0.007353\n",
       "olhar                                                        0.006863\n",
       "jogar                                                        0.006863\n",
       "gente                                                        0.006863\n",
       "respondanome                                                 0.006863\n",
       "vale                                                         0.006863\n",
       "stop                                                         0.006863\n",
       "brancofruta                                                  0.006373\n",
       "no                                                           0.006373\n",
       "                                                               ...   \n",
       "objeto                                                       0.000490\n",
       "outro                                                        0.000490\n",
       "chegando                                                     0.000490\n",
       "ver                                                          0.000490\n",
       "breno                                                        0.000490\n",
       "polícia                                                      0.000490\n",
       "tomar                                                        0.000490\n",
       "porshe                                                       0.000490\n",
       "birra                                                        0.000490\n",
       "vermelhobebida                                               0.000490\n",
       "vapeboy                                                      0.000490\n",
       "ônibus                                                       0.000490\n",
       "véi                                                          0.000490\n",
       "bárbaraberlimbourbonbeijinhobugattibegebananabatutabarata    0.000490\n",
       "ofereci                                                      0.000490\n",
       "blindado                                                     0.000490\n",
       "veyront                                                      0.000490\n",
       "precisa                                                      0.000490\n",
       "turbo                                                        0.000490\n",
       "noteikkkkkkkkkkkk                                            0.000490\n",
       "venceu                                                       0.000490\n",
       "dai                                                          0.000490\n",
       "bacalhau                                                     0.000490\n",
       "bolonhesacarro                                               0.000490\n",
       "quero                                                        0.000490\n",
       "9685                                                         0.000490\n",
       "tiro                                                         0.000490\n",
       "lhes                                                         0.000490\n",
       "fbxlbsdt6b                                                   0.000490\n",
       "palace                                                       0.000490\n",
       "Length: 892, dtype: float64"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_rel_palavras_nao_relevantes_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montando o Classificardor Naive Bayes\n",
    "\n",
    "#### Probabilidades de ser relevante ou não relevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3416"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_palavras_teste = palavras_relevantes_teste.size + palavras_nao_relevantes_teste.size\n",
    "num_palavras_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probabilidade do tweet ser relevante\n",
    "prob_relevante_teste = tweets_relevante_teste.size / tweets_universo_teste.size\n",
    "prob_relevante_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probabilidade do tweet não ser relevante\n",
    "prob_nao_relevante_teste = tweets_nao_relevante_teste.size / tweets_universo_teste.size\n",
    "prob_nao_relevante_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como o classificador funcionará:\n",
    "\n",
    "Todos os tweets presentes na planilha de testes serão, individualmente, analisados pelo classificador e catalogados, em um loop. Depois de todo esse processo, os resultados de cada tweet analisado, que podem ser ou \"Relevante\" ou \"Irrelevante\", pelo menos neste primeiro momento, serão adicionados a uma lista com o fim de serem comparados com a análise feita à mão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef Bayes(relevancia,serie1,serie2):\\n    #Criando o loop para o cálculo da probabilidade de ser relevante:\\n    i = 0 \\n    lista_resultados_numericos = []\\n\\n    #selecionando o tweet\\n    for tweets in testudo:\\n        \\n        #transformando em lista de str\\n        tweetr = tweets.split()\\n        \\n        # Calculando a probabilidade deste tweet ser relevante dado as suas palavras\\n        log_probabilidade = 0\\n        for palavra in tweetr:\\n        \\n            #Não deixa que palavras não pertencentes à série bugem o código\\n            if palavra not in serie1:\\n                log_probabilidade += math.log(1/3416, 10)\\n            else:\\n                log_probabilidade += math.log(serie2[palavra] + 1/3416, 10)\\n        \\n        #Guardando os resultados e resetando a variável probabilidade    \\n        lista_resultados_numericos.append(log_probabilidade + math.log(relevancia, 10))\\n        log_probabilidade = 0\\n        i += 1\\n\\n    return lista_resultados_numerico'\\n\""
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def Bayes(relevancia,serie1,serie2):\n",
    "    #Criando o loop para o cálculo da probabilidade de ser relevante:\n",
    "    i = 0 \n",
    "    lista_resultados_numericos = []\n",
    "\n",
    "    #selecionando o tweet\n",
    "    for tweets in testudo:\n",
    "        \n",
    "        #transformando em lista de str\n",
    "        tweetr = tweets.split()\n",
    "        \n",
    "        # Calculando a probabilidade deste tweet ser relevante dado as suas palavras\n",
    "        log_probabilidade = 0\n",
    "        for palavra in tweetr:\n",
    "        \n",
    "            #Não deixa que palavras não pertencentes à série bugem o código\n",
    "            if palavra not in serie1:\n",
    "                log_probabilidade += math.log(1/3416, 10)\n",
    "            else:\n",
    "                log_probabilidade += math.log(serie2[palavra] + 1/3416, 10)\n",
    "        \n",
    "        #Guardando os resultados e resetando a variável probabilidade    \n",
    "        lista_resultados_numericos.append(log_probabilidade + math.log(relevancia, 10))\n",
    "        log_probabilidade = 0\n",
    "        i += 1\n",
    "\n",
    "    return lista_resultados_numerico'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1e-10 #é uma estimativa e deve mudar\n",
    "vocabulario = 356000\n",
    "\n",
    "# defini probabilidade de ser relevante:\n",
    "def relevante(tweet):\n",
    "    \n",
    "    tweet_relevante_4 = tweet.split()\n",
    "\n",
    "    prob = 1\n",
    "    \n",
    "    for p in tweet_relevante_4:\n",
    "        \n",
    "        if p in freq_rel_palavras_relevantes_teste:\n",
    "            prob = prob*(freq_rel_palavras_relevantes_teste[p]*palavras_relevantes_teste.size+smooth)/(palavras_relevantes_teste.size+smooth*vocabulario)*1000\n",
    "        else:\n",
    "            prob = prob*(smooth)/(palavras_relevantes_teste.size+smooth*vocabulario)*1000\n",
    "            \n",
    "    prob = prob * prob_relevante_teste\n",
    "    prob = math.log10(prob)\n",
    "    #print(prob)\n",
    "    return prob\n",
    "\n",
    "# defini probabilidade de ser irrelevante:\n",
    "def irrelevante(tweet):\n",
    "    \n",
    "    tweet_irrelevante_4 = tweet.split()\n",
    "            \n",
    "    prob = 1\n",
    "    \n",
    "    for p in tweet_irrelevante_4:\n",
    "        \n",
    "        if p in freq_rel_palavras_nao_relevantes_teste:\n",
    "            prob = prob*(freq_rel_palavras_nao_relevantes_teste[p]*palavras_nao_relevantes_teste.size+smooth)/(palavras_nao_relevantes_teste.size+smooth*vocabulario)*1000\n",
    "        else:\n",
    "            prob = prob*(smooth)/(palavras_nao_relevantes_teste.size+smooth*vocabulario)*1000\n",
    "            \n",
    "    prob = prob * prob_nao_relevante_teste\n",
    "    prob = math.log10(prob)\n",
    "    #print(prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "lista_resultados = []\n",
    "for tweet in tweets_universo_teste:\n",
    "    \n",
    "    if relevante(tweet) > irrelevante(tweet):\n",
    "           lista_resultados.append(1)\n",
    "            \n",
    "    if relevante(tweet) < irrelevante(tweet):\n",
    "        lista_resultados.append(0)\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "lista_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "0\n",
      "124\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "verdadeiros_positivos = 0\n",
    "falsos_positivos = 0\n",
    "verdadeiros_negativos = 0\n",
    "falsos_negativos = 0\n",
    "\n",
    "for tweet in raw2['Relevância']:\n",
    "    if tweet == lista_resultados[i] and lista_resultados[i] == 1:\n",
    "        verdadeiros_positivos += 1\n",
    "    \n",
    "    if tweet == lista_resultados[i] and lista_resultados[i] == 0:\n",
    "        verdadeiros_negativos += 1\n",
    "    \n",
    "    if tweet != lista_resultados[i] and lista_resultados[i] == 1:\n",
    "        falso_positivos += 1\n",
    "        \n",
    "    if tweet != lista_resultados[i] and lista_resultados[i] == 0:\n",
    "        falsos_negativos += 1\n",
    "        \n",
    "    i += 1\n",
    "        \n",
    "print(verdadeiros_positivos)\n",
    "print(falsos_positivos)\n",
    "print(verdadeiros_negativos)\n",
    "print(falsos_negativos)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperfeiçoamento:\n",
    "\n",
    "Os trabalhos vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separação de espaços entre palavras e emojis ou emojis e emojis\n",
    "* Propor outras limpezas e transformações que não afetem a qualidade da informação ou classificação\n",
    "* Criar categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza análise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
